Model Optimizer :
The Model Optimizer helps convert models in multiple different frameworks to an Intermediate Representation, which is used with the Inference Engine
----------------------------------------------------------------------------------------------------------------------------
Intel Openvino supported frameworks:
Caffe - UC berkely
tensorflow - gooogle brains
MXnet - Apache 
ONNXX - Facebook and Apple ML
Kaldi - Individual(Speech Recognition)
----------------------------------------------------------------------------------------------------------------------------
Model optimizer to Inference engine :
The IR is able to be loaded directly into the Inference Engine, and is actually made of two output files from the Model Optimizer: an XML file and a binary file. The XML file holds the model architecture and other important metadata, while the binary file holds weights and biases in a binary format. You need both of these files in order to run inference 
----------------------------------------------------------------------------------------------------------------------------
